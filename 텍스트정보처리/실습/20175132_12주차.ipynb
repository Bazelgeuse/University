{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d4fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.9.0 in c:\\users\\98psw\\appdata\\roaming\\python\\python39\\site-packages (0.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from torchtext==0.9.0) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from torchtext==0.9.0) (1.21.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from torchtext==0.9.0) (4.64.0)\n",
      "Requirement already satisfied: torch==1.8.0 in c:\\users\\98psw\\appdata\\roaming\\python\\python39\\site-packages (from torchtext==0.9.0) (1.8.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from torch==1.8.0->torchtext==0.9.0) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from requests->torchtext==0.9.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from requests->torchtext==0.9.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from requests->torchtext==0.9.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from requests->torchtext==0.9.0) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\98psw\\anaconda3\\lib\\site-packages (from tqdm->torchtext==0.9.0) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.9.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e1bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torchtext.legacy import data\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb951fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e8ff5026d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669d64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_SIZE = 5\n",
    "\n",
    "def tokenizer(text):\n",
    "    token = kkma.morphs(text)\n",
    "    if len(token) < KERNEL_SIZE:\n",
    "            for i in range(0, KERNEL_SIZE-len(token)):\n",
    "                token.append('<PAD>')\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3c3e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 279\n",
      "테스트 샘플의 개수 : 99\n",
      "{'review': ['페이스', '허', '거', '같', '음', 'ㅋㅋㅋㅋㅋ'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "REVIEW = data.Field(tokenize = tokenizer, batch_first = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "fields = {'review': ('review', REVIEW), 'label': ('label',LABEL)}\n",
    "\n",
    "train_data, test_data = data.TabularDataset.splits(path = './', train = 'train_data.csv', test = 'test_data.csv',format = 'csv', fields = fields)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))\n",
    "print(vars(train_data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7998037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 1002\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x000001E8A6650550>>, {'<unk>': 0, '<pad>': 1, '이': 2, '하': 3, 'ㄴ': 4, '는': 5, '.': 6, '고': 7, '영화': 8, '다': 9, '보': 10, '도': 11, '었': 12, '어': 13, '아': 14, '의': 15, '은': 16, '에': 17, '을': 18, '..': 19, '...': 20, 'ㄹ': 21, '가': 22, '<PAD>': 23, '았': 24, '들': 25, '있': 26, '것': 27, '지': 28, ',': 29, '없': 30, '나': 31, '게': 32, '음': 33, '더': 34, '를': 35, '정말': 36, '는데': 37, '!': 38, '네요': 39, '안': 40, '기': 41, '만': 42, '?': 43, 'ㅁ': 44, '거': 45, 'ㄴ다': 46, '같': 47, '면': 48, '않': 49, '어요': 50, '에서': 51, '연기': 52, '너무': 53, '되': 54, '로': 55, '....': 56, '싶': 57, 'ㅂ니다': 58, '진짜': 59, '수': 60, '어서': 61, '재밌': 62, '나오': 63, '라': 64, '만들': 65, '아니': 66, '알': 67, '잘': 68, '점': 69, '좋': 70, '~': 71, '감동': 72, '내용': 73, '네': 74, '습니다': 75, '왜': 76, '주': 77, '남': 78, '완전': 79, '좀': 80, '지만': 81, 'ㄴ데': 82, '감독': 83, '겠': 84, '때': 85, '말': 86, '모르': 87, '못': 88, '사람': 89, '아서': 90, '오': 91, '요': 92, '으로': 93, '줄': 94, '최고': 95, \"'\": 96, '-': 97, 'ㄴ가': 98, '건': 99, '과': 100, '그': 101, '냐': 102, '배우': 103, '서': 104, '이런': 105, '재미없': 106, '중': 107, '지루': 108, '그냥': 109, '내': 110, '느낌': 111, '드라마': 112, '듯': 113, '라고': 114, '시간': 115, '와': 116, '재미': 117, '재미있': 118, '절대': 119, '하나': 120, '한': 121, '10': 122, '3': 123, '^^': 124, 'ㅋㅋ': 125, '기대': 126, '년': 127, '노': 128, '니': 129, '두': 130, '또': 131, '별로': 132, '보다': 133, '생각': 134, '스토리': 135, '아쉽': 136, '야': 137, '이것': 138, '인생': 139, '작품': 140, '전': 141, '죽': 142, '1': 143, '??': 144, 'ㄴ지': 145, 'ㄹ까': 146, 'ㅋ': 147, 'ㅡㅡ': 148, '까': 149, '는지': 150, '다시': 151, '대': 152, '레': 153, '리': 154, '만하': 155, '면서': 156, '못하': 157, '속': 158, '아깝': 159, '어도': 160, '역시': 161, '으면': 162, '이야기': 163, '전개': 164, '!!': 165, '!!!': 166, '2': 167, 'ㅋㅋㅋ': 168, 'ㅜㅜ': 169, 'ㅠㅠ': 170, '갈': 171, '구': 172, '나름': 173, '낳': 174, '내가': 175, '님': 176, '다고': 177, '다는': 178, '대하': 179, '데': 180, '많': 181, '무섭': 182, '받': 183, '발': 184, '보이': 185, '비': 186, '살': 187, '쓰레기': 188, '아도': 189, '어야': 190, '용': 191, '은데': 192, '을까': 193, '잘하': 194, '전혀': 195, '정도': 196, '조금': 197, '주인공': 198, '(': 199, ')': 200, '.....': 201, ';;': 202, '개': 203, '구나': 204, '그렇': 205, '그리고': 206, '기억': 207, '끄': 208, '나이': 209, '드': 210, '따뜻': 211, '류': 212, '머': 213, '명작': 214, '모습': 215, '몰입': 216, '뭐': 217, '밋': 218, '바': 219, '베': 220, '별': 221, '뻔': 222, '사랑': 223, '살인': 224, '소재': 225, '솔직히': 226, '스럽': 227, '시': 228, '실망': 229, '실제': 230, '애': 231, '액션': 232, '어리': 233, '웃': 234, '웃기': 235, '이상': 236, '일본': 237, '임': 238, '자': 239, '자신': 240, '자체': 241, '재': 242, '잼': 243, '저': 244, '지네': 245, '진심': 246, '짱': 247, '초': 248, '최악': 249, '터': 250, '판': 251, '해지': 252, '%': 253, ';': 254, '???': 255, 'OOO': 256, '~~': 257, 'ㄴ가요': 258, 'ㅇ': 259, '가슴': 260, '갈등': 261, '갈수록': 262, '검술': 263, '그런': 264, '그리': 265, '그리하': 266, '그만': 267, '극장': 268, '긴장감': 269, '김': 270, '끝나': 271, '나도': 272, '너무나': 273, '놈': 274, '누': 275, '느끼': 276, '니까': 277, '답': 278, '당시': 279, '더라': 280, '던': 281, '둘': 282, '듣': 283, '디': 284, '딩': 285, '뜨': 286, '라도': 287, '란': 288, '려고': 289, '르': 290, '마지막': 291, '막': 292, '많이': 293, '멋지': 294, '무슨': 295, '문제': 296, '버리': 297, '별루': 298, '보고': 299, '볼만': 300, '비디오': 301, '빠지': 302, '세': 303, '스': 304, '쓰': 305, '아름답': 306, '아요': 307, '아주': 308, '악역': 309, '어떻': 310, '언제': 311, '없이': 312, '여': 313, '연': 314, '위하': 315, '음식': 316, '음악': 317, '이거': 318, '이쁘': 319, '이유': 320, '잊': 321, '작': 322, '잔인': 323, '장면': 324, '장애': 325, '적': 326, '적인': 327, '정신': 328, '제일': 329, '조연': 330, '중간': 331, '지금': 332, '질': 333, '짜증': 334, '차': 335, '참': 336, '초반': 337, '코미디': 338, '키': 339, '티비': 340, '편': 341, '평범': 342, '평점': 343, '한번': 344, '햏햏': 345, '헐': 346, '화': 347, '힘들': 348, '+': 349, '......': 350, '0': 351, '4': 352, '8': 353, '98': 354, ';;;': 355, '♥': 356, 'ㄴ다고': 357, 'ㄹ수록': 358, 'ㅈ': 359, 'ㅉ': 360, 'ㅋㅋㅋㅋ': 361, 'ㅎ': 362, 'ㅎㅎ': 363, 'ㅎㅎㅎ': 364, 'ㅠ': 365, '가볍': 366, '가장': 367, '가지': 368, '감성': 369, '강하': 370, '강호': 371, '개그': 372, '개연성': 373, '겁': 374, '게이': 375, '결국': 376, '계속': 377, '고민': 378, '고요': 379, '곤': 380, '골': 381, '공감': 382, '관객': 383, '괜찮': 384, '국산': 385, '군': 386, '군더더기': 387, '그대로': 388, '그러나': 389, '그저': 390, '극': 391, '근데': 392, '기분': 393, '기사': 394, '기에': 395, '기자': 396, '깔끔': 397, '깨알': 398, '꼭': 399, '꽤': 400, '꽤나': 401, '꿈': 402, '끌': 403, '나쁘': 404, '남기': 405, '내내': 406, '넘': 407, '년도': 408, '높': 409, '놓': 410, '눈': 411, '느리': 412, '느와': 413, '늘': 414, '니스': 415, '다가': 416, '다니': 417, '단순': 418, '단연': 419, '당하': 420, '닿': 421, '대단': 422, '대박': 423, '대체': 424, '댓': 425, '더빙': 426, '돈': 427, '돋보이': 428, '뒤': 429, '딱': 430, '때리': 431, '떼': 432, '뛰어나': 433, '라는': 434, '라니': 435, '랑': 436, '려': 437, '로맨스': 438, '리스트': 439, '마': 440, '만찬': 441, '만큼': 442, '말하': 443, '망': 444, '망치': 445, '매력': 446, '매칭': 447, '맨': 448, '맨날': 449, '먹': 450, '멀': 451, '멜로': 452, '몇': 453, '모든': 454, '모순': 455, '목소리': 456, '무엇': 457, '물': 458, '물건': 459, '뭔': 460, '뮤지컬': 461, '미': 462, '밖에': 463, '백': 464, '버킷': 465, '범죄': 466, '보기': 467, '보여주': 468, '본인': 469, '부분': 470, '부족': 471, '부터': 472, '불륜': 473, '비정': 474, '빌리': 475, '뻑': 476, '뻔하': 477, '뿐': 478, '사': 479, '사진': 480, '살리': 481, '상상': 482, '상황': 483, '설정': 484, '성': 485, '세상': 486, '소리': 487, '소설': 488, '손': 489, '송': 490, '수작': 491, '수준': 492, '순간': 493, '순위': 494, '슬픔': 495, '시리즈': 496, '시선': 497, '시절': 498, '심': 499, '심심': 500, '심하': 501, '싸우': 502, '쏙': 503, '씩': 504, '아끼': 505, '아무': 506, '아물': 507, '아야': 508, '아이': 509, '아직': 510, '알바': 511, '어거지': 512, '어떤': 513, '어야지': 514, '어울리': 515, '억지': 516, '엉망': 517, '여도': 518, '여운': 519, '여자애': 520, '연기력': 521, '오버': 522, '완전히': 523, '왠': 524, '외국': 525, '요즘': 526, '욕하': 527, '우': 528, '우리': 529, '울': 530, '웃음': 531, '유': 532, '유명': 533, '유치': 534, '은은': 535, '이기': 536, '이나': 537, '이랑': 538, '이렇게': 539, '이민기': 540, '이제': 541, '이틀': 542, '이해': 543, '익숙': 544, '인간': 545, '있음': 546, '자극적': 547, '잔잔': 548, '잠': 549, '재탕': 550, '적당히': 551, '전달': 552, '전쟁': 553, '제대로': 554, '조작': 555, '조폭': 556, '좋아하': 557, '죄인': 558, '죠': 559, '주년': 560, '주인': 561, '주제': 562, '죽이': 563, '지겹': 564, '지나': 565, '지존': 566, '직전': 567, '진부': 568, '짜증나': 569, '짬뽕': 570, '쩔': 571, '찍': 572, '차이': 573, '처음': 574, '초등학교': 575, '최': 576, '추': 577, '추억': 578, '치': 579, '카': 580, '캐릭터': 581, '캐스팅': 582, '케이블': 583, '콘서트': 584, '크': 585, '타': 586, '탓': 587, '태어나': 588, '트의': 589, '파': 590, '판타지': 591, '패': 592, '평': 593, '평론가': 594, '포스': 595, '포스터': 596, '피': 597, '하지만': 598, '학년': 599, '한국': 600, '한마디': 601, '한이': 602, '행복': 603, '허': 604, '허무': 605, '현실': 606, '형': 607, '혜': 608, '화려': 609, '훈': 610, '훈훈': 611, '훨': 612, '흠': 613, '흥행': 614, '히': 615, '!!!!': 616, ',,': 617, ',,,': 618, ',,,,': 619, '-2': 620, '-_-': 621, '.......': 622, '........': 623, '/': 624, '1.2': 625, '12': 626, '14': 627, '1997': 628, '20': 629, '2009': 630, '2010': 631, '50': 632, '9': 633, ':': 634, 'C': 635, 'CG': 636, 'CGV': 637, 'D': 638, 'DVD': 639, 'MBC': 640, 'TV': 641, '^^;': 642, '`': 643, '~~~~~~~~~~~': 644, 'ㄱ': 645, 'ㄴ걸요': 646, 'ㄴ다지만': 647, 'ㄷ': 648, 'ㄹ게요': 649, 'ㄹ라': 650, 'ㄹ라고': 651, 'ㄹ지': 652, 'ㄹ지라도': 653, 'ㅄ': 654, 'ㅋㅋㅋㅋㅋ': 655, 'ㅋㅋㅋㅋㅋㅋㅋ': 656, 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ': 657, 'ㅋ기네비어역': 658, 'ㅔ': 659, 'ㅜ': 660, '가구': 661, '가르': 662, '가문': 663, '가발': 664, '가시': 665, '가요': 666, '가져가': 667, '각기': 668, '간': 669, '감각': 670, '감기': 671, '감도': 672, '감동적': 673, '감정': 674, '감히': 675, '강': 676, '강수': 677, '강추': 678, '강한': 679, '개가': 680, '개봉': 681, '개인적': 682, '개진': 683, '거도': 684, '거부': 685, '거슬리': 686, '거지': 687, '거칠': 688, '건너': 689, '건담': 690, '건지': 691, '걸리': 692, '걸어다니': 693, '걸음마': 694, '검사': 695, '게임': 696, '겟다': 697, '결말': 698, '결코': 699, '계': 700, '고위': 701, '고은': 702, '고자': 703, '고지': 704, '고추': 705, '공유': 706, '공인': 707, '공주': 708, '공짜': 709, '광고': 710, '광장': 711, '교도소': 712, '교훈': 713, '교훈적': 714, '구경': 715, '구리': 716, '구성': 717, '군가': 718, '군요': 719, '굳굳': 720, '굳이': 721, '굶주리': 722, '굿': 723, '굿바이': 724, '규': 725, '그것': 726, '그나마': 727, '그냥저냥': 728, '그때': 729, '그래서': 730, '그래픽': 731, '그러': 732, '그럴싸하': 733, '그릇': 734, '극적': 735, '근친': 736, '글': 737, '금물': 738, '급': 739, '긍정': 740, '기가': 741, '기도': 742, '기명': 743, '기본': 744, '기준': 745, '긴장': 746, '긷': 747, '길': 748, '길래': 749, '깨': 750, '깨닫': 751, '꺼': 752, '꺼지': 753, '꼽': 754, '꽃': 755, '끊': 756, '끌어가': 757, '끝': 758, '끝내': 759, '끼': 760, '나가': 761, '나중': 762, '낚이': 763, '남발': 764, '남자': 765, '납득': 766, '낫': 767, '낭비': 768, '낮': 769, '내고': 770, '내생': 771, '내일': 772, '내전': 773, '내주': 774, '냥': 775, '너무너무': 776, '넘어지': 777, '넘쳐나': 778, '넣': 779, '노골적': 780, '노래': 781, '노출': 782, '녹아들': 783, '녹음': 784, '놀': 785, '놀아나': 786, '누이': 787, '눈부시': 788, '느': 789, '는군': 790, '늙': 791, '늦': 792, '다듬': 793, '다른': 794, '다리': 795, '다면': 796, '다미': 797, '다소': 798, '다이': 799, '다크': 800, '다투': 801, '다하': 802, '단지': 803, '단하': 804, '달': 805, '달려가': 806, '달밤': 807, '달팽이': 808, '닭': 809, '닮': 810, '담기': 811, '담백': 812, '답답': 813, '당': 814, '당황': 815, '대본': 816, '대유': 817, '대작': 818, '대적': 819, '대주': 820, '대한민국': 821, '대희': 822, '더군': 823, '더니': 824, '더라면': 825, '더럽': 826, '던데': 827, '던스트': 828, '던지': 829, '덜': 830, '뎅': 831, '도가': 832, '도움': 833, '돌아가시': 834, '돌아오': 835, '돌이': 836, '동서양': 837, '동심': 838, '동화': 839, '돼': 840, '돼지': 841, '둑': 842, '뒷받침': 843, '드래곤': 844, '드럽': 845, '듣기': 846, '들어가': 847, '들어맞': 848, '들어주': 849, '들이': 850, '듯하': 851, '따': 852, '따라가': 853, '따로': 854, '따르': 855, '딱히': 856, '딸리': 857, '때려잡': 858, '때문': 859, '땡칠이': 860, '떠들': 861, '떠오르': 862, '떨': 863, '떨어지': 864, '똥': 865, '뛰': 866, '뛰쳐나오': 867, '뜻': 868, '뜻밖': 869, '라구요': 870, '라미': 871, '라파스': 872, '락스코': 873, '란슬롯': 874, '랜슬롯으': 875, '랫': 876, '랬으': 877, '량': 878, '러': 879, '럼': 880, '려구': 881, '려는': 882, '로맨틱': 883, '로봇': 884, '로서': 885, '로써': 886, '룡': 887, '르덴': 888, '리스': 889, '리얼': 890, '리얼리티': 891, '리플릿': 892, '링': 893, '마라': 894, '마르소': 895, '마세': 896, '마스코트': 897, '마음': 898, '마이너스': 899, '마저': 900, '마치': 901, '막장': 902, '만약': 903, '만이': 904, '말리': 905, '말씀드리': 906, '맘마': 907, '맛': 908, '망하': 909, '맞': 910, '매': 911, '매끄럽': 912, '매니저': 913, '매력적': 914, '매번': 915, '매운맛': 916, '맥스': 917, '머금': 918, '먹먹': 919, '먹먹하': 920, '멀리': 921, '멋': 922, '멋있': 923, '메': 924, '메세지': 925, '메인': 926, '며': 927, '면상': 928, '명': 929, '명대사': 930, '명치': 931, '명품': 932, '모건': 933, '모녀': 934, '모으': 935, '모자라': 936, '목': 937, '몬스터': 938, '몰': 939, '묘사': 940, '무': 941, '무겁': 942, '무난': 943, '무당': 944, '무서웠어요ㅠ탑': 945, '무한': 946, '문자': 947, '문제자': 948, '문학적': 949, '묻': 950, '물론': 951, '뭉클': 952, '뭐하': 953, '뭥': 954, '뮤': 955, '미국': 956, '미국인': 957, '미모': 958, '미소': 959, '미스': 960, '미아': 961, '믹스': 962, '민망': 963, '민폐': 964, '믿': 965, '밀': 966, '밋밋': 967, '밍': 968, '바꿔주': 969, '바라': 970, '바라보': 971, '바래': 972, '바른': 973, '바스코': 974, '바이러스': 975, '박정': 976, '반개': 977, '발견': 978, '발톱': 979, '밝': 980, '밤': 981, '방': 982, '방송': 983, '배': 984, '배꼽': 985, '배역': 986, '버무러': 987, '범수': 988, '범죄자': 989, '벗어나': 990, '베테랑': 991, '벨': 992, '변명': 993, '복': 994, '본지': 995, '볼': 996, '봉': 997, '봐주': 998, '부': 999, '부르': 1000, '부성애': 1001})\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "REVIEW.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, vectors = 'fasttext.simple.300d', unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(REVIEW.vocab)))\n",
    "print(REVIEW.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec01e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size = BATCH_SIZE, sort_key = lambda x: len(x.review), sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "825ef7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, embedding_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        embed = self.dropout(self.embedding(x))\n",
    "        output, _ = self.rnn(embed)\n",
    "        output = self.linear(output[:, -1, :])\n",
    "        return output\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db41571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(REVIEW.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b243f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(INPUT_DIM, 32, OUTPUT_DIM, EMBEDDING_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c9cb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6123816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b30257da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.review).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a4022e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25f31a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, critertion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.review).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3154e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.687 | Train Acc: 54.22%\n",
      "\t Val. Loss: 0.673 |  Val. Acc: 61.26%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.680 | Train Acc: 62.30%\n",
      "\t Val. Loss: 0.663 |  Val. Acc: 65.77%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.678 | Train Acc: 61.36%\n",
      "\t Val. Loss: 0.656 |  Val. Acc: 64.38%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.681 | Train Acc: 56.50%\n",
      "\t Val. Loss: 0.646 |  Val. Acc: 66.47%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.671 | Train Acc: 59.77%\n",
      "\t Val. Loss: 0.635 |  Val. Acc: 71.23%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, train_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2194aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_kernels, kernel_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, out_channels = n_kernels, kernel_size = (ksize, embedding_dim)) for ksize in kernel_sizes])\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*n_kernels, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, review):\n",
    "        embedded = self.embedding(review)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        res = self.fc(cat)\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8ec79e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 파라미터 수 :  336661\n"
     ]
    }
   ],
   "source": [
    "N_KERNELS = 10\n",
    "KERNEL_SIZES = [3,4,5]\n",
    "PAD_IDX = REVIEW.vocab.stoi[REVIEW.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_KERNELS, KERNEL_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "print('모델 파라미터 수 : ', sum(param.numel() for param in model.parameters() if param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a63b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = REVIEW.vocab.stoi[REVIEW.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73bd8693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.727 | Train Acc: 53.42%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 52.13%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.744 | Train Acc: 46.97%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.09%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.729 | Train Acc: 54.66%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.44%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.702 | Train Acc: 52.13%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 52.13%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.715 | Train Acc: 53.17%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.40%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, train_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c985d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
